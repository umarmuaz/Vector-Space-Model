{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vector Space Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi2ONZtLCumT"
      },
      "source": [
        "# Vector Space Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80TRjcJrAoEN",
        "outputId": "50404c0f-ea6c-4a72-ef3d-322e745b7a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import glob\n",
        "import nltk\n",
        "nltk.download('popular');\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from collections import Counter \n",
        "from collections import defaultdict\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQbcl1u1DGyl"
      },
      "source": [
        "## Import and Extract Data from Google Drive to current working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBABjOPABPy1",
        "outputId": "c2c9a57e-6d90-4a40-a4c5-adde22c05706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# importing required modules \n",
        "from zipfile import ZipFile \n",
        "path=\"/content/drive/My Drive/IR_A1/\"\n",
        "# specifying the zip file name and path\n",
        "file_name = path+\"ACL txt.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip1: \n",
        "    # extracting all the files in current folder\n",
        "    print('Extracting all the files now...') \n",
        "    zip1.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8D80AGJDf3n"
      },
      "source": [
        "## Read files, remove punctuations and stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6CJn1FpBuLc"
      },
      "source": [
        "def wordList_removePuncs(fld_path):                             #give path of the folder containing all documents\n",
        "    dic = {}\n",
        "    stop_words = set(stopwords.words('english')) # All stop words\n",
        "    file_names = glob.glob(fld_path)  # Read all names of documents in the list\n",
        "    for file in tqdm(file_names,position=0):\n",
        "        name = file.split('/')[-1]\n",
        "        with open(file, 'r', errors='ignore') as f: #ignore files with errors\n",
        "            x = f.read()\n",
        "        x = x.split() # split in list\n",
        "        x = [''.join(c for c in s if c not in string.punctuation) for s in x] #remove punctuations\n",
        "        x = [w for w in x if not w in stop_words and len(w) >= 3 and w.isalpha()] #remove stop words, digits and 2 letter words\n",
        "        x = [s for s in x if s] #Remove empty list elements\n",
        "        dic[name] =' '.join(x)  #Join list to make a string\n",
        "    return dic"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQZuTbWkFzrw"
      },
      "source": [
        "## Calculate Term frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOSlxxZjCEKI"
      },
      "source": [
        "def termFrequencyInDoc(doc_dict):\n",
        "    tf_docs = {}  # create a dict for storing term frequency in document\n",
        "    tf_docs = {doc: {} for doc in doc_dict.keys()}\n",
        "    for doc_id,doc in tqdm(doc_dict.items(),position=0):\n",
        "        tf_docs[doc_id] = dict(Counter(doc.lower().split()))\n",
        "    return tf_docs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAmNT3ntGH8Y"
      },
      "source": [
        "## Calculate Word Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLQjaqtXCJZD"
      },
      "source": [
        "def wordDocFre(doc_dict):\n",
        "  DF = defaultdict(int) # create a default dict for storing word document frequency\n",
        "  for doc in tqdm(doc_dict.values(),position=0):\n",
        "      words = doc.lower().split()\n",
        "      for word in set(words):\n",
        "        DF[word] += 1  # defaultdict simplifies \"if key in word_idf: ...\" part\n",
        "  return DF"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgzvR_r6GeEs"
      },
      "source": [
        "## Calculate Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqPvmLITCMb0"
      },
      "source": [
        "def inverseDocFre(doc_fre,length):\n",
        "    idf= {} # create a dict for storing inverse word document frequency\n",
        "    for word in list(df.keys()):     \n",
        "        idf[word] = np.log2((length+1) / doc_fre[word])\n",
        "    return idf"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoJX39uIGs_9"
      },
      "source": [
        "## Calculate TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhSTt8PfCXAy"
      },
      "source": [
        "def tfidf(tf,idf_scr,doc_dict):\n",
        "    tf_idf_scr = {} # create a dict for storing tf-idf values\n",
        "    tf_idf_scr={doc: {} for doc in doc_dict.keys()}\n",
        "    for doc_id,doc in tqdm(doc_dict.items(),position=0):\n",
        "      for word in doc.lower().split():\n",
        "        tf_idf_scr[doc_id][word] = tf[doc_id].get(word, 0) * idf_scr[word]\n",
        "    return tf_idf_scr"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P500ZNrdHBaT"
      },
      "source": [
        "## Vector Space Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3lN_KhnCbDe"
      },
      "source": [
        "def vectorSpaceModel(query, doc_dict,tfidf_scr):\n",
        "    query_vocab = [] # create a list for storing query vocabulary\n",
        "    for word in query.lower().split():\n",
        "        if word not in query_vocab:\n",
        "            query_vocab.append(word)\n",
        "\n",
        "    query_wc = {} # create a dict for storing query word count\n",
        "    for word in query_vocab:\n",
        "        query_wc[word] = query.lower().split().count(word)\n",
        "    \n",
        "    relevance_scores = {} # create a dict for storing relevance scores for query words\n",
        "    for doc_id in doc_dict.keys():\n",
        "        score = 0\n",
        "        for word in query_vocab:\n",
        "            try:\n",
        "              score += query_wc[word] * tfidf_scr[doc_id][word]\n",
        "            except:\n",
        "              continue\n",
        "        relevance_scores[doc_id] = score\n",
        "        #sort relevance scores and return top 5 for the query.\n",
        "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
        "    top_5 = {k: sorted_value[k] for k in list(sorted_value)[:5]}\n",
        "    return top_5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ZFciVPHXzT"
      },
      "source": [
        "## Retreive Top 5 Query Relevent Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5npgdtAyCgDI",
        "outputId": "35322582-9e0f-4071-e930-ca51dcc34d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t0 = time.time()                       #starting time\n",
        "doc_dict=wordList_removePuncs('/content/ACL txt/*.txt')     #return document dictionary         \n",
        "tf=termFrequencyInDoc( doc_dict)       #return term frequency\n",
        "df=wordDocFre(doc_dict)                #return word document frequency\n",
        "M = len(doc_dict)                      #Total number of documents\n",
        "idf_dict = inverseDocFre(df,M)         #returns idf scores\n",
        "tf_idf = tfidf(tf,idf_dict,doc_dict)   #returns tf-idf socres\n",
        "\n",
        "#All Query Words\n",
        "query1 = 'LDA'\n",
        "query2 = 'Topic modelling'\n",
        "query3 = 'Generative models'\n",
        "query4 = 'Semantic relationships between terms'\n",
        "query5 = 'Natural Language Processing'\n",
        "query6 = 'Text Mining'\n",
        "query7 = 'Translation model'\n",
        "query8 = 'Learning procedures for the lexicon'\n",
        "query9 = 'Semantic evaluations'\n",
        "query10 = 'System results and combination'\n",
        "\n",
        "#Top 5 documents for all Queries using Vector Space Model\n",
        "top1 = vectorSpaceModel(query1, doc_dict,tf_idf)    \n",
        "top2 = vectorSpaceModel(query2, doc_dict,tf_idf)    \n",
        "top3 = vectorSpaceModel(query3, doc_dict,tf_idf)    \n",
        "top4 = vectorSpaceModel(query4, doc_dict,tf_idf)    \n",
        "top5 = vectorSpaceModel(query5, doc_dict,tf_idf)   \n",
        "top6 = vectorSpaceModel(query6, doc_dict,tf_idf)  \n",
        "top7 = vectorSpaceModel(query7, doc_dict,tf_idf)  \n",
        "top8 = vectorSpaceModel(query8, doc_dict,tf_idf)  \n",
        "top9 = vectorSpaceModel(query9, doc_dict,tf_idf)   \n",
        "top10 = vectorSpaceModel(query10, doc_dict,tf_idf)   \n",
        "\n",
        "t1 = time.time() #Ending Time\n",
        "print('\\n')\n",
        "print('Total time of execution : ',str(t1-t0),' seconds') #print total time for program execution\n",
        "print('\\n')\n",
        "#Print top 5 documents for all queries\n",
        "print('Top 5 Documents for LDA: \\n', list(top1))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Topic modelling: \\n', list(top2))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Generative models: \\n', list(top3))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Semantic relationships between terms: \\n', list(top4))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Natural Language Processing: \\n', list(top5))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Text Mining: \\n', list(top6))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Translation model: \\n', list(top7))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Learning procedures for the lexicon: \\n', list(top8))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Semantic evaluations: \\n', list(top9))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for System results and combination: \\n', list(top10))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21941/21941 [02:29<00:00, 146.60it/s]\n",
            "100%|██████████| 21941/21941 [00:13<00:00, 1667.16it/s]\n",
            "100%|██████████| 21941/21941 [00:17<00:00, 1219.43it/s]\n",
            "100%|██████████| 21941/21941 [00:52<00:00, 418.46it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total time of execution :  235.8566906452179  seconds\n",
            "\n",
            "\n",
            "Top 5 Documents for LDA: \n",
            " ['J14-2003.pdf.txt', 'D09-1026.pdf.txt', 'D11-1050.pdf.txt', 'N10-1070.pdf.txt', 'P10-1117.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Topic modelling: \n",
            " ['J14-2003.pdf.txt', 'P12-1079.pdf.txt', 'Q15-1004.pdf.txt', 'N15-1074.pdf.txt', 'W10-4104.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Generative models: \n",
            " ['W06-1668.pdf.txt', 'W11-0100.pdf.txt', 'J03-4003.pdf.txt', 'D09-1111.pdf.txt', 'D09-1058.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Semantic relationships between terms: \n",
            " ['W11-0100.pdf.txt', 'J08-2004.pdf.txt', 'W15-3808.pdf.txt', 'W09-2004.pdf.txt', 'J09-2003.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Natural Language Processing: \n",
            " ['W11-0100.pdf.txt', 'J14-1005.pdf.txt', 'J87-1020.pdf.txt', 'W14-55.x.pdf.txt', 'J86-2001.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Text Mining: \n",
            " ['D09-1162.pdf.txt', 'P06-1062.pdf.txt', 'P12-1062.pdf.txt', 'W09-2609.pdf.txt', 'P09-1098.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Translation model: \n",
            " ['J85-2006.pdf.txt', 'J03-3003.pdf.txt', 'J06-4004.pdf.txt', 'W14-3302.pdf.txt', 'J03-3004.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Learning procedures for the lexicon: \n",
            " ['W11-0100.pdf.txt', 'J87-3007.pdf.txt', 'W06-1647.pdf.txt', 'W10-2505.pdf.txt', 'J87-3009.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for Semantic evaluations: \n",
            " ['W11-0100.pdf.txt', 'J08-2004.pdf.txt', 'J09-4008.pdf.txt', 'J09-2003.pdf.txt', 'J91-1003.pdf.txt']\n",
            "\n",
            "\n",
            "Top 5 Documents for System results and combination: \n",
            " ['W11-0100.pdf.txt', 'J01-2002.pdf.txt', 'N10-1141.pdf.txt', 'P11-1127.pdf.txt', 'J11-3003.pdf.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65nAQ8oMHubw"
      },
      "source": [
        "## Document names with their Relevance Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RwQFXwbCmBC",
        "outputId": "80cc41c0-f468-4eb6-daa5-ad2d3cf0a533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Document Names with their Relevance Scores\\n')\n",
        "print('Top 5 Documents for LDA: \\n', list(top1.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Topic modelling: \\n', list(top2.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Generative models: \\n', list(top3.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Semantic relationships between terms: \\n', list(top4.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Natural Language Processing: \\n', list(top5.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Text Mining: \\n', list(top6.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Translation model: \\n', list(top7.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Learning procedures for the lexicon: \\n', list(top8.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for Semantic evaluations: \\n', list(top9.items()))\n",
        "print('\\n')\n",
        "print('Top 5 Documents for System results and combination: \\n', list(top10.items()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document Names with their Relevance Scores\n",
            "\n",
            "Top 5 Documents for LDA: \n",
            " [('J14-2003.pdf.txt', 381.11829876445427), ('D09-1026.pdf.txt', 351.8015065518039), ('D11-1050.pdf.txt', 342.0292424809205), ('N10-1070.pdf.txt', 312.7124502682702), ('P10-1117.pdf.txt', 298.05405416194503)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Topic modelling: \n",
            " [('J14-2003.pdf.txt', 375.99023016444085), ('P12-1079.pdf.txt', 335.6400591224033), ('Q15-1004.pdf.txt', 300.7921841315527), ('N15-1074.pdf.txt', 298.9580854478237), ('W10-4104.pdf.txt', 278.7829999268049)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Generative models: \n",
            " [('W06-1668.pdf.txt', 187.482857604789), ('W11-0100.pdf.txt', 184.37811943982004), ('J03-4003.pdf.txt', 151.6418584247996), ('D09-1111.pdf.txt', 132.91153581921373), ('D09-1058.pdf.txt', 132.148611029166)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Semantic relationships between terms: \n",
            " [('W11-0100.pdf.txt', 1010.6154364316802), ('J08-2004.pdf.txt', 200.0156245489287), ('W15-3808.pdf.txt', 182.73011439548486), ('W09-2004.pdf.txt', 171.03532256989146), ('J09-2003.pdf.txt', 163.89416395977642)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Natural Language Processing: \n",
            " [('W11-0100.pdf.txt', 154.63456382331844), ('J14-1005.pdf.txt', 90.61202394855985), ('J87-1020.pdf.txt', 86.10066643242732), ('W14-55.x.pdf.txt', 61.82688508880327), ('J86-2001.pdf.txt', 55.806476954426834)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Text Mining: \n",
            " [('D09-1162.pdf.txt', 163.07850107122383), ('P06-1062.pdf.txt', 162.72294033282247), ('P12-1062.pdf.txt', 123.5430725665296), ('W09-2609.pdf.txt', 120.23363849370503), ('P09-1098.pdf.txt', 111.16233058829981)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Translation model: \n",
            " [('J85-2006.pdf.txt', 448.2669608250973), ('J03-3003.pdf.txt', 419.2116925496081), ('J06-4004.pdf.txt', 338.3075588996698), ('W14-3302.pdf.txt', 327.77942356018445), ('J03-3004.pdf.txt', 326.9760788139946)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Learning procedures for the lexicon: \n",
            " [('W11-0100.pdf.txt', 366.76980750823424), ('J87-3007.pdf.txt', 217.05424051256344), ('W06-1647.pdf.txt', 202.9091888451578), ('W10-2505.pdf.txt', 198.2835247402), ('J87-3009.pdf.txt', 191.62163368869253)]\n",
            "\n",
            "\n",
            "Top 5 Documents for Semantic evaluations: \n",
            " [('W11-0100.pdf.txt', 867.704482439134), ('J08-2004.pdf.txt', 199.21156980645387), ('J09-4008.pdf.txt', 194.14731606784173), ('J09-2003.pdf.txt', 160.67794498987703), ('J91-1003.pdf.txt', 124.32546874782341)]\n",
            "\n",
            "\n",
            "Top 5 Documents for System results and combination: \n",
            " [('W11-0100.pdf.txt', 232.43242279095847), ('J01-2002.pdf.txt', 154.17489069241248), ('N10-1141.pdf.txt', 152.2123608286384), ('P11-1127.pdf.txt', 129.10539951233733), ('J11-3003.pdf.txt', 128.21176694223567)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kROAm3J7H6L5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}